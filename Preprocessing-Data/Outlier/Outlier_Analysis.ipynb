{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f924700",
   "metadata": {},
   "source": [
    "# Question A\n",
    "\n",
    "## What automated approaches can you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45a720e",
   "metadata": {},
   "source": [
    "For automated approaches to data quality assessment, you can use several methods:\n",
    "\n",
    "#### Checking for Missing Values: \n",
    "\n",
    "    Automatically scan datasets to identify and handle missing data points.\n",
    "\n",
    "#### Identifying Data Type Inconsistencies:\n",
    "\n",
    "    Employ algorithms to detect fields that do not conform to expected data types.\n",
    "\n",
    "#### Outlier Detection:\n",
    "\n",
    "    Implement statistical methods to identify and possibly rectify data points that deviate significantly from the norm.\n",
    "\n",
    "#### Treatment Statistical Methods:\n",
    "\n",
    "    Apply automated techniques to correct issues such as outliers or inconsistencies, enhancing overall data quality.\n",
    "\n",
    "Automating these processes reduces the need for manual decision-making and expedites the data cleaning and preparation phase, ensuring data quality efficiently and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cafceb",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcca6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083f023",
   "metadata": {},
   "source": [
    "# Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"E:\\VamStar\\AA.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ecee93",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485903c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # data types\n",
    "print(\"Original data types:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ababe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial data info:\")\n",
    "print(data['winner_price'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a1d234",
   "metadata": {},
   "source": [
    "> ## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values count:\", data['winner_price'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c1382",
   "metadata": {},
   "source": [
    ">## Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_count = data.duplicated().sum()\n",
    "print(\"Duplicate values count:\", duplicate_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8318ab90",
   "metadata": {},
   "source": [
    "# EDA Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc4317f",
   "metadata": {},
   "source": [
    ">## Finding Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c6d7e3",
   "metadata": {},
   "source": [
    ">>  ### Using BoxPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc59471",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20, 8))  # Increased size to make each subplot \n",
    "\n",
    "# First subplot: Boxplot\n",
    "plt.subplot(1, 2, 1)  \n",
    "sns.boxplot(y=data['winner_price'])\n",
    "plt.title('Boxplot of Winner Price')\n",
    "plt.ylabel('Winner Price')\n",
    "plt.grid(True)\n",
    "\n",
    "# Second subplot: Histogram with KDE\n",
    "plt.subplot(1, 2, 2) \n",
    "sns.histplot(data['winner_price'], kde=True, bins=15)\n",
    "plt.title('Distribution of Winner Price')\n",
    "plt.xlabel('Winner Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the entire figure with both plots\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98465667",
   "metadata": {},
   "source": [
    ">>  ### Using IQR - Trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e1b77",
   "metadata": {},
   "source": [
    "Outliers can sometimes be removed without concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c5788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate Q1, Q3, and IQR\n",
    "Q1 = data['winner_price'].quantile(0.25)\n",
    "Q3 = data['winner_price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Determine outliers using IQR\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c959c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the lower and upper bounds to 5 decimal places\n",
    "lower_bound_rounded = round(lower_bound, 5)\n",
    "upper_bound_rounded = round(upper_bound, 5)\n",
    "\n",
    "# Print the rounded lower and upper bounds\n",
    "print(\"Lower Bound:\", lower_bound_rounded)\n",
    "print(\"Upper Bound:\", upper_bound_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the outliers to keep only non-outliers\n",
    "data_clean_Trimming = data[(data['winner_price'] >= lower_bound) & (data['winner_price'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6752883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8)) \n",
    "\n",
    "# First subplot: Boxplot\n",
    "plt.subplot(1, 2, 1)  \n",
    "sns.boxplot(y=data_clean_Trimming['winner_price'])\n",
    "plt.title('Boxplot of Winner Price')\n",
    "plt.ylabel('Winner Price')\n",
    "plt.grid(True)\n",
    "\n",
    "# Second subplot: Histogram with KDE\n",
    "plt.subplot(1, 2, 2)  \n",
    "sns.histplot(data_clean_Trimming['winner_price'], kde=True, bins=15)\n",
    "plt.title('Distribution of Winner Price')\n",
    "plt.xlabel('Winner Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the entire figure with both plots\n",
    "plt.tight_layout()  # Adjusts subplots to give some padding and prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac325ce5",
   "metadata": {},
   "source": [
    ">>  ### Using IQR - Capping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649a3f6",
   "metadata": {},
   "source": [
    "\n",
    "Sometimes deleting data due to outliers can lead to the loss of other useful features, so using the capping method may be a preferable alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2485440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping outliers\n",
    "data_clean_Capping = data['winner_price'].apply(\n",
    "    lambda x: max(min(x, upper_bound), lower_bound)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a44e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))  \n",
    "\n",
    "# First subplot: Boxplot\n",
    "plt.subplot(1, 2, 1)  \n",
    "sns.boxplot(y=data_clean_Capping)\n",
    "plt.title('Boxplot of Winner Price')\n",
    "plt.ylabel('Winner Price')\n",
    "plt.grid(True)\n",
    "\n",
    "# Second subplot: Histogram with KDE\n",
    "plt.subplot(1, 2, 2)  \n",
    "sns.histplot(data_clean_Capping, kde=True, bins=15)\n",
    "plt.title('Distribution of Winner Price')\n",
    "plt.xlabel('Winner Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the entire figure with both plots\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fcd42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.kdeplot(data['winner_price'], label='Original', color=\"blue\", linestyle=':')\n",
    "sns.kdeplot(data_clean_Trimming['winner_price'], label='Trimmed', color=\"red\")\n",
    "sns.kdeplot(data['capped_winner_price'], label='Capped', color=\"green\", linestyle='--')\n",
    "\n",
    "plt.title('Comparison of Winner Price Distribution: Original, Trimmed, and Capped')\n",
    "plt.xlabel('Winner Price')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac26b5",
   "metadata": {},
   "source": [
    "The comparison between the original, trimmed, and capped distributions of winner_price illustrates the effects of each outlier handling method. Trimming removes outliers, resulting in a more compact and possibly normal distribution, ideal for analyses sensitive to extreme values. In contrast, capping truncates the distribution's tails and introduces peaks at the bounds, maintaining data integrity but potentially affecting natural distribution interpretations. Each method suits different analytical needs; trimming is useful for reducing data variability, while capping preserves dataset size, beneficial when data loss is a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658dd0e",
   "metadata": {},
   "source": [
    "# Question B\n",
    "\n",
    "## What manual tasks would you perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6d144",
   "metadata": {},
   "source": [
    "#### Sample Review:\n",
    "Even after applying automated cleaning procedures, manually review a random sample of the data to ensure that the data looks reasonable and the automated steps have worked as intended.\n",
    "#### Anomaly Investigation: \n",
    "When outliers are detected by automated methods, manually inspect these cases to determine if they are true anomalies or legitimate extreme values. This often requires deeper business knowledge or additional context that might not be captured in the data alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edb1af",
   "metadata": {},
   "source": [
    "# Question C\n",
    "\n",
    "## How would you improve this process long term and how would you build your roadmap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1bbb2f",
   "metadata": {},
   "source": [
    "#### Automation of Data Cleansing:\n",
    "    Develop and refine scripts and algorithms that automate the steps outlined above, reducing the need for manual intervention and increasing efficiency.\n",
    "#### Regular Audits and Updates:\n",
    "    Set up a schedule for regular audits of the data cleansing process to ensure its ongoing relevance and effectiveness as the data evolves.\n",
    "#### Integration with Data Sources:\n",
    "    Work towards tighter integration with data sources to improve data accuracy at the point of entry. This could involve improving data collection methods or initial data validation steps.\n",
    "#### Scaling the Process:\n",
    "    As the process scales from a few SKUs to hundreds or thousands, consider leveraging cloud-based data platforms that can handle large volumes of data and provide powerful tools for data processing and machine learning.\n",
    "#### Use of Advanced Tools:\n",
    "    Adopt more sophisticated data quality and anomaly detection tools that can scale with increasing data size and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855bdb01",
   "metadata": {},
   "source": [
    "condensed version of the roadmap for enhancing your data cleansing processes:\n",
    "\n",
    "### Phase 1: Planning and Setup\n",
    "#### Define Objectives: \n",
    "Set clear goals for data quality and identify key data sources.\n",
    "#### Tool Selection:\n",
    "Choose appropriate tools for data cleansing and integration that are scalable.\n",
    "### Phase 2: Development and Integration\n",
    "#### Develop Automation Scripts:\n",
    "Write scripts to automate data cleansing tasks like error correction and data standardization.\n",
    "#### Integrate with Data Sources: \n",
    "Enhance data accuracy at entry points and streamline data collection methods.\n",
    "### Phase 3: Deployment and Scaling\n",
    "#### Deploy Automation Tools:\n",
    "Implement the automated data cleansing scripts.\n",
    "#### Initial Scaling:\n",
    "Use cloud platforms for handling larger datasets and start scaling the processes.\n",
    "### Phase 4: Maintenance and Auditing\n",
    "#### Regular Audits:\n",
    "Establish a schedule for regular audits to evaluate the process effectiveness.\n",
    "#### Update Processes: \n",
    "Refine and update tools and scripts based on audit findings.\n",
    "### Phase 5: Advanced Scaling and Sophistication\n",
    "#### Adopt Advanced Tools:\n",
    "Integrate sophisticated data quality and anomaly detection tools.\n",
    "#### Comprehensive Scaling:\n",
    "Expand cloud usage to support larger data volumes and complexity.\n",
    "### Phase 6: Continuous Improvement\n",
    "#### Monitor and Adapt:\n",
    "Continuously monitor processes and update strategies based on new technology and business needs.\n",
    "\n",
    "This streamlined roadmap focuses on critical steps that ensure effective data cleansing and management, leading to improved data quality and operational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacf044",
   "metadata": {},
   "source": [
    "# Question D\n",
    "\n",
    "## Would you change anything if you would need to scale this process from a few SKU's to hundreds and thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468fa55",
   "metadata": {},
   "source": [
    "\n",
    "Considering the substantial volume of information we manage, it is advisable to avoid aggregating multiple data points within single fields (e.g., combining different attributes in fields like participants, participants_price'). This approach can complicate the use of methods such as one-hot encoding, which in turn increases both the total amount of information and the number of features. It is recommended to structure data by ensuring that each attribute is stored in distinct fields to maintain clarity and efficiency in data management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
